{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part II: Variational AutoEncoder (MNIST data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# We need to install the package 'torchvision' >> pip install torchvision\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code can run on gpu (or) cpu, we can use the gpu if available. \n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64         # number of data points in each batch\n",
    "N_EPOCHS = 10           # times to run the model on complete data\n",
    "INPUT_DIM = 28 * 28     # size of each input\n",
    "HIDDEN_DIM = 256        # hidden dimension\n",
    "LATENT_DIM = 50         # latent vector dimension\n",
    "lr = 1e-3               # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To transform images into Tensors\n",
    "transforms2 = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "train_dataset = datasets.MNIST(\n",
    "    './data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms2)\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    './data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms2\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    def __init__(self, input_dim, hidden_dim, latent_dim):\n",
    "        \n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            z_dim: A integer indicating the latent dimension.\n",
    "        '''\n",
    "        \n",
    "        super( Encoder, self).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, z_dim)\n",
    "        self.var = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x is of shape [batch_size, input_dim]\n",
    "        hidden = F.relu(self.linear(x))\n",
    "        \n",
    "        # hidden is of shape [batch_size, hidden_dim]\n",
    "        z_mu = self.mu(hidden)\n",
    "        \n",
    "        # z_mu is of shape [batch_size, latent_dim]\n",
    "        z_var = self.var(hidden)\n",
    "        \n",
    "        # z_var is of shape [batch_size, latent_dim]\n",
    "        return z_mu, z_var\n",
    "\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "\n",
    "    def __init__(self, z_dim, hidden_dim, output_dim):\n",
    "        \n",
    "        '''\n",
    "        Args:\n",
    "            z_dim: A integer indicating the latent size.\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            output_dim: A integer indicating the output dimension (in case of MNIST it is 28 * 28)\n",
    "        '''\n",
    "        \n",
    "        super( Decoder, self ).__init__()\n",
    "\n",
    "        self.linear = nn.Linear(z_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # x is of shape [batch_size, latent_dim]\n",
    "        hidden = F.relu(self.linear(x))\n",
    "        \n",
    "        # hidden is of shape [batch_size, hidden_dim]\n",
    "        predicted = torch.sigmoid(self.out(hidden))\n",
    "        \n",
    "        # predicted is of shape [batch_size, output_dim]\n",
    "        return predicted\n",
    "\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    \n",
    "    def __init__(self, enc, dec):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # encode\n",
    "        z_mu, z_var = self.enc(x)\n",
    "\n",
    "        # sample from the distribution having latent parameters z_mu, z_var\n",
    "        # reparameterize\n",
    "        std = torch.exp(z_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_sample = eps.mul(std).add_(z_mu)\n",
    "\n",
    "        # decode\n",
    "        predicted = self.dec(x_sample)\n",
    "        return predicted, z_mu, z_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, LATENT_DIM)\n",
    "\n",
    "# decoder\n",
    "decoder = Decoder(LATENT_DIM, HIDDEN_DIM, INPUT_DIM)\n",
    "\n",
    "# vae\n",
    "model = VAE(encoder, decoder).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # set the train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (x, _) in enumerate(train_iterator):\n",
    "        \n",
    "        # reshape the data into [batch_size, 784]\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = x.to(device)\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        x_sample, z_mu, z_var = model(x)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
    "\n",
    "        # kl divergence loss\n",
    "        kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "        # total loss\n",
    "        loss = recon_loss + kl_loss\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    \n",
    "    # set the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # test loss for the data\n",
    "    test_loss = 0\n",
    "\n",
    "    # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(test_iterator):\n",
    "            \n",
    "            # reshape the data\n",
    "            x = x.view(-1, 28 * 28)\n",
    "            x = x.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            x_sample, z_mu, z_var = model(x)\n",
    "\n",
    "            # reconstruction loss\n",
    "            recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
    "\n",
    "            # kl divergence loss\n",
    "            kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "            # total loss\n",
    "            loss = recon_loss + kl_loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabim\\Anaconda3\\lib\\site-packages\\torch\\nn\\_reduction.py:49: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> epoch  0\n",
      "---> Train loss: >>  159.784177742513\n",
      "-----> Test loss: >>  129.30947602539064\n",
      "\n",
      ">> epoch  1\n",
      "---> Train loss: >>  122.81843084309895\n",
      "-----> Test loss: >>  116.60857385253907\n",
      "\n",
      ">> epoch  2\n",
      "---> Train loss: >>  114.99634158528646\n",
      "-----> Test loss: >>  112.3841458618164\n",
      "\n",
      ">> epoch  3\n",
      "---> Train loss: >>  111.97773973795573\n",
      "-----> Test loss: >>  110.07647175292969\n",
      "\n",
      ">> epoch  4\n",
      "---> Train loss: >>  110.26603181559244\n",
      "-----> Test loss: >>  109.00849635009766\n",
      "\n",
      ">> epoch  5\n",
      "---> Train loss: >>  109.15387970784505\n",
      "-----> Test loss: >>  108.12953454589844\n",
      "\n",
      ">> epoch  6\n",
      "---> Train loss: >>  108.34749813639323\n",
      "-----> Test loss: >>  107.22363530273438\n",
      "\n",
      ">> epoch  7\n",
      "---> Train loss: >>  107.83285013020833\n",
      "-----> Test loss: >>  106.96130755615235\n",
      "\n",
      ">> epoch  8\n",
      "---> Train loss: >>  107.35418283691406\n",
      "-----> Test loss: >>  106.89969051513673\n",
      "\n",
      ">> epoch  9\n",
      "---> Train loss: >>  106.96202411295573\n",
      "-----> Test loss: >>  106.58446188964844\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    test_loss /= len(test_dataset)\n",
    "    \n",
    "    print('>> epoch ', epoch)\n",
    "    print('---> Train loss: >> ', train_loss)\n",
    "    print('-----> Test loss: >> ', test_loss)\n",
    "    print('')\n",
    "\n",
    "    if best_test_loss > test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        patience_counter = 1\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 50])\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Choose an image to generate and plot\n",
    "z = torch.randn(1, LATENT_DIM).to(device)\n",
    "reconstructed_img = model.dec(z)\n",
    "img = reconstructed_img.view(28, 28).data\n",
    "\n",
    "print(z.shape)\n",
    "print(img.shape)\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
